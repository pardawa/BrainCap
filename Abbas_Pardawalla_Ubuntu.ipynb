{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6682aa6c",
   "metadata": {},
   "source": [
    "# UBUNTU DataSet - Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3307dbe2",
   "metadata": {},
   "source": [
    "Geared towards understanding the content within it, in order to create a customer service NLP chatbot in the hopes of providing answers to users in seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e9414",
   "metadata": {},
   "source": [
    "The data provided below are multiple questions geared by past users on frequent problems and setbacks occored while using the platform. With the help of understanding those questions and subsequent answers provided, we should be able to create a simple bot that helps users getting answers to Frequently Asked Questions in a matter of miliseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b927c",
   "metadata": {},
   "source": [
    "FYI: As the datset files are really big in size, I am working on them individually by file in order to increase my run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd836d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firstly importing all libraries, to make subsequent queries run efficently\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#These are just the main files needed to proceed further. Subsequent libraries if required will be ran on a need basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7257cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      lines      words  characters              filename\n",
      "0   9212878   91660344   996253904  dialogueText_196.csv\n",
      "1  16587831  166392849  1799936480  dialogueText_301.csv\n",
      "2   1038325   11035331   116070597      dialogueText.csv\n"
     ]
    }
   ],
   "source": [
    "#Reading the main file to see the contents\n",
    "ubuntu = pd.read_csv('./archive_2/toc.csv')\n",
    "print(ubuntu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f89547",
   "metadata": {},
   "source": [
    "Lets do an analysis of whats included in each of the three above files. Then based on the findings, we can decide which file to use to further our object "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f66ca",
   "metadata": {},
   "source": [
    "Lets get acquanited with our datasets and see what they actually entail. We will run an info section below to find out the datatypes in each file and total range of entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa7de4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>dialogueID</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>126125.tsv</td>\n",
       "      <td>2008-04-23T14:55:00.000Z</td>\n",
       "      <td>bad_image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hello folks, please help me a bit with the fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>126125.tsv</td>\n",
       "      <td>2008-04-23T14:56:00.000Z</td>\n",
       "      <td>bad_image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Did I choose a bad channel? I ask because you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>126125.tsv</td>\n",
       "      <td>2008-04-23T14:57:00.000Z</td>\n",
       "      <td>lordleemo</td>\n",
       "      <td>bad_image</td>\n",
       "      <td>the second sentence is better english   and we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>64545.tsv</td>\n",
       "      <td>2009-08-01T06:22:00.000Z</td>\n",
       "      <td>mechtech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sock Puppe?t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>64545.tsv</td>\n",
       "      <td>2009-08-01T06:22:00.000Z</td>\n",
       "      <td>mechtech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WTF?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   folder  dialogueID                      date       from         to  \\\n",
       "0       3  126125.tsv  2008-04-23T14:55:00.000Z  bad_image        NaN   \n",
       "1       3  126125.tsv  2008-04-23T14:56:00.000Z  bad_image        NaN   \n",
       "2       3  126125.tsv  2008-04-23T14:57:00.000Z  lordleemo  bad_image   \n",
       "3       3   64545.tsv  2009-08-01T06:22:00.000Z   mechtech        NaN   \n",
       "4       3   64545.tsv  2009-08-01T06:22:00.000Z   mechtech        NaN   \n",
       "\n",
       "                                                text  \n",
       "0  Hello folks, please help me a bit with the fol...  \n",
       "1  Did I choose a bad channel? I ask because you ...  \n",
       "2  the second sentence is better english   and we...  \n",
       "3                                       Sock Puppe?t  \n",
       "4                                               WTF?  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ubun1 = pd.read_csv('./archive_2/dialogue_texts/dialogueText.csv')\n",
    "ubun1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a401ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>dialogueID</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9212872</th>\n",
       "      <td>13</td>\n",
       "      <td>3676.tsv</td>\n",
       "      <td>2012-07-07T20:17:00.000Z</td>\n",
       "      <td>MonkeyDust</td>\n",
       "      <td>legolas</td>\n",
       "      <td>= arian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9212873</th>\n",
       "      <td>13</td>\n",
       "      <td>3676.tsv</td>\n",
       "      <td>2012-07-07T20:18:00.000Z</td>\n",
       "      <td>MonkeyDust</td>\n",
       "      <td>legolas</td>\n",
       "      <td>observation and deduction, dear watson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9212874</th>\n",
       "      <td>13</td>\n",
       "      <td>16586.tsv</td>\n",
       "      <td>2008-07-25T01:53:00.000Z</td>\n",
       "      <td>linuxfce</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am trying to install nvidia drivers from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9212875</th>\n",
       "      <td>13</td>\n",
       "      <td>16586.tsv</td>\n",
       "      <td>2008-07-25T01:53:00.000Z</td>\n",
       "      <td>linuxfce</td>\n",
       "      <td>NaN</td>\n",
       "      <td>how do i enter runlevel 3? when i try telinit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9212876</th>\n",
       "      <td>13</td>\n",
       "      <td>16586.tsv</td>\n",
       "      <td>2008-07-25T01:54:00.000Z</td>\n",
       "      <td>linuxfce</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anyone know how to enter runlevel 3 in ubuntu?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         folder dialogueID                      date        from       to  \\\n",
       "9212872      13   3676.tsv  2012-07-07T20:17:00.000Z  MonkeyDust  legolas   \n",
       "9212873      13   3676.tsv  2012-07-07T20:18:00.000Z  MonkeyDust  legolas   \n",
       "9212874      13  16586.tsv  2008-07-25T01:53:00.000Z    linuxfce      NaN   \n",
       "9212875      13  16586.tsv  2008-07-25T01:53:00.000Z    linuxfce      NaN   \n",
       "9212876      13  16586.tsv  2008-07-25T01:54:00.000Z    linuxfce      NaN   \n",
       "\n",
       "                                                      text  \n",
       "9212872                                            = arian  \n",
       "9212873             observation and deduction, dear watson  \n",
       "9212874  i am trying to install nvidia drivers from the...  \n",
       "9212875  how do i enter runlevel 3? when i try telinit ...  \n",
       "9212876     anyone know how to enter runlevel 3 in ubuntu?  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ubun2 = pd.read_csv('./archive_2/dialogue_texts/dialogueText_196.csv')\n",
    "ubun2.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c98ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>dialogueID</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16587825</th>\n",
       "      <td>32</td>\n",
       "      <td>1783.tsv</td>\n",
       "      <td>2007-11-15T03:38:00.000Z</td>\n",
       "      <td>koyo001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16587826</th>\n",
       "      <td>32</td>\n",
       "      <td>1783.tsv</td>\n",
       "      <td>2007-11-15T03:39:00.000Z</td>\n",
       "      <td>koyo001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>does anyone know something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16587827</th>\n",
       "      <td>32</td>\n",
       "      <td>1783.tsv</td>\n",
       "      <td>2007-11-15T03:39:00.000Z</td>\n",
       "      <td>neverblue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no, no one knows everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16587828</th>\n",
       "      <td>32</td>\n",
       "      <td>1783.tsv</td>\n",
       "      <td>2007-11-15T03:40:00.000Z</td>\n",
       "      <td>koyo001</td>\n",
       "      <td>ikonia</td>\n",
       "      <td>the camera doesnt work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16587829</th>\n",
       "      <td>32</td>\n",
       "      <td>1783.tsv</td>\n",
       "      <td>2007-11-15T03:40:00.000Z</td>\n",
       "      <td>neverblue</td>\n",
       "      <td>koyo001</td>\n",
       "      <td>I believe you missed a post or two while you w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          folder dialogueID                      date       from       to  \\\n",
       "16587825      32   1783.tsv  2007-11-15T03:38:00.000Z    koyo001      NaN   \n",
       "16587826      32   1783.tsv  2007-11-15T03:39:00.000Z    koyo001      NaN   \n",
       "16587827      32   1783.tsv  2007-11-15T03:39:00.000Z  neverblue      NaN   \n",
       "16587828      32   1783.tsv  2007-11-15T03:40:00.000Z    koyo001   ikonia   \n",
       "16587829      32   1783.tsv  2007-11-15T03:40:00.000Z  neverblue  koyo001   \n",
       "\n",
       "                                                       text  \n",
       "16587825                                             thanks  \n",
       "16587826                         does anyone know something  \n",
       "16587827                        no, no one knows everything  \n",
       "16587828                             the camera doesnt work  \n",
       "16587829  I believe you missed a post or two while you w...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ubun3 = pd.read_csv('./archive_2/dialogue_texts/dialogueText_301.csv')\n",
    "ubun3.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f7a0f",
   "metadata": {},
   "source": [
    "Next lets find out the details of each data set to what type of data is used in the set and how we can use them. And if need be, to change some of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "793dc843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info of First DataSet\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1038324 entries, 0 to 1038323\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count    Dtype \n",
      "---  ------      --------------    ----- \n",
      " 0   folder      1038324 non-null  int64 \n",
      " 1   dialogueID  1038324 non-null  object\n",
      " 2   date        1038324 non-null  object\n",
      " 3   from        1038311 non-null  object\n",
      " 4   to          566035 non-null   object\n",
      " 5   text        1038237 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 47.5+ MB\n",
      "None\n",
      "\n",
      "Info of Second DataSet\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9212877 entries, 0 to 9212876\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Dtype \n",
      "---  ------      ----- \n",
      " 0   folder      int64 \n",
      " 1   dialogueID  object\n",
      " 2   date        object\n",
      " 3   from        object\n",
      " 4   to          object\n",
      " 5   text        object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 421.7+ MB\n",
      "None\n",
      "\n",
      "Info of Third DataSet\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16587830 entries, 0 to 16587829\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Dtype \n",
      "---  ------      ----- \n",
      " 0   folder      int64 \n",
      " 1   dialogueID  object\n",
      " 2   date        object\n",
      " 3   from        object\n",
      " 4   to          object\n",
      " 5   text        object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 759.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Info of First DataSet')\n",
    "print(ubun1.info())\n",
    "print('\\nInfo of Second DataSet')\n",
    "print(ubun2.info())\n",
    "print('\\nInfo of Third DataSet')\n",
    "print(ubun3.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f080577",
   "metadata": {},
   "source": [
    "So from our observations we can see that most of the data types are what they have to be, instead of the date column. It is currently an object type, and for further analysis, we will need to change it to datetime object so easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7ecd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of First DataSet\n",
      "folder        1038324\n",
      "dialogueID    1038324\n",
      "date          1038324\n",
      "from          1038311\n",
      "to             566035\n",
      "text          1038237\n",
      "dtype: int64\n",
      "\n",
      "Count of Second DataSet\n",
      "folder        9212877\n",
      "dialogueID    9212877\n",
      "date          9212877\n",
      "from          9212675\n",
      "to            5917560\n",
      "text          9212063\n",
      "dtype: int64\n",
      "\n",
      "Count of Third DataSet\n",
      "folder        16587830\n",
      "dialogueID    16587830\n",
      "date          16587830\n",
      "from          16587543\n",
      "to            10853175\n",
      "text          16586581\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Count of First DataSet')\n",
    "print(ubun1.count())\n",
    "print('\\nCount of Second DataSet')\n",
    "print(ubun2.count())\n",
    "print('\\nCount of Third DataSet')\n",
    "print(ubun3.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965cce03",
   "metadata": {},
   "source": [
    "We know the number of total values in each dataset, and based on that can know how many are null and do not need be in our set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98667c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['folder', 'dialogueID', 'date', 'from', 'to', 'text'], dtype='object')\n",
      "Index(['folder', 'dialogueID', 'date', 'from', 'to', 'text'], dtype='object')\n",
      "Index(['folder', 'dialogueID', 'date', 'from', 'to', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Getting a feel of the number of columns to see if they are similar and can be joined\n",
    "print(ubun1.columns)\n",
    "print(ubun2.columns)\n",
    "print(ubun3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33d67baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "folder             0\n",
       "dialogueID         0\n",
       "date               0\n",
       "from              13\n",
       "to            472289\n",
       "text              87\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding all null values in our first dataset\n",
    "ubun1.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855c4d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "folder              0\n",
       "dialogueID          0\n",
       "date                0\n",
       "from              202\n",
       "to            3295317\n",
       "text              814\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding all null values in our second dataset\n",
    "ubun2.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb986b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "folder              0\n",
       "dialogueID          0\n",
       "date                0\n",
       "from              287\n",
       "to            5734655\n",
       "text             1249\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding all null values in our third dataset\n",
    "ubun3.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a097de",
   "metadata": {},
   "source": [
    "As the number of null values in to section is really high, lets look to see if they will make an impact in the total number of values and what percentage is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d3d33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Values NaN in 'to' column from ubun1 = 16.561873382388015\n",
      "Percentage of Values NaN in 'to' column from ubun2 = 44.31290937480989\n",
      "Percentage of Values NaN in 'to' column from ubun3 = 47.16149882407682\n"
     ]
    }
   ],
   "source": [
    "#Percentage of missing values in the to section\n",
    "total_to = ubun1['to'].count()\n",
    "missing_to = ubun1['to'].isna().sum()\n",
    "difference_to = total_to - missing_to\n",
    "\n",
    "percentage = (difference_to/total_to)*100\n",
    "print(f\"Percentage of Values NaN in 'to' column from ubun1 = {percentage}\")\n",
    "\n",
    "#Percentage of missing values in the to section\n",
    "total_to2 = ubun2['to'].count()\n",
    "missing_to2 = ubun2['to'].isna().sum()\n",
    "difference_to2 = total_to2 - missing_to2\n",
    "\n",
    "percentage2 = (difference_to2/total_to2)*100\n",
    "print(f\"Percentage of Values NaN in 'to' column from ubun2 = {percentage2}\")\n",
    "\n",
    "#Percentage of missing values in the to section\n",
    "total_to3 = ubun3['to'].count()\n",
    "missing_to3 = ubun3['to'].isna().sum()\n",
    "difference_to3 = total_to3 - missing_to3\n",
    "\n",
    "percentage3 = (difference_to3/total_to3)*100\n",
    "print(f\"Percentage of Values NaN in 'to' column from ubun3 = {percentage3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07a752",
   "metadata": {},
   "source": [
    "Above percentage values indicate that in couple of our files, we are missing a huge chunk of usernames. This will need further clarification and to see if they can be looked at or replaced with.<br>The main thing is the info in the text column that we will be needing in order to train our bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39812aec",
   "metadata": {},
   "source": [
    "We know that the conversations are had by anonymous users, lets see if we can break down to see the number of unique users who are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4f9385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users in Ubun1 Dataset:165020\n",
      "Number of unique users in Ubun2 Dataset:255573\n",
      "Number of unique users in Ubun3 Dataset:318406\n"
     ]
    }
   ],
   "source": [
    "unique_users1 = pd.concat([ubun1['from'], ubun1['to']]).nunique()\n",
    "print(f\"Number of unique users in Ubun1 Dataset:{unique_users1}\")\n",
    "\n",
    "unique_users2 = pd.concat([ubun2['from'], ubun2['to']]).nunique()\n",
    "print(f\"Number of unique users in Ubun2 Dataset:{unique_users2}\")\n",
    "\n",
    "unique_users3 = pd.concat([ubun3['from'], ubun3['to']]).nunique()\n",
    "print(f\"Number of unique users in Ubun3 Dataset:{unique_users3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071dc7c6",
   "metadata": {},
   "source": [
    "Okay data is never clean and there are always some lines that are duplicated. Lets see how many rows in each data set are duplicated and thus with the process of elimanation we can choose to remove and not work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14ced6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows in ubun1 data set = 1640\n",
      "Number of duplicated rows in ubun2 data set = 25161\n",
      "Number of duplicated rows in ubun3 data set = 44872\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of duplicated rows in ubun1 data set = {ubun1.duplicated().sum()}')\n",
    "print(f'Number of duplicated rows in ubun2 data set = {ubun2.duplicated().sum()}')\n",
    "print(f'Number of duplicated rows in ubun3 data set = {ubun3.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336fdaf4",
   "metadata": {},
   "source": [
    "Compared to the total number of rows, the number isnt as significant, so removing them will still leave us with a decent number of lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e1afa",
   "metadata": {},
   "source": [
    "As the text in the datasets is heavily relevant on replies from users, lets go through each data set and find out the most active users on this site.<br> What this will help us with, is to use the text data from them to come creat the top FAQ answers for our bot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7654f8",
   "metadata": {},
   "source": [
    "Each user id, is its own voice and personality. With that we can have several different feels of different people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e81acaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 User with Questions:\n",
      "ubuntu           1578\n",
      "jrib             1342\n",
      "Pici             1289\n",
      "bazhang          1276\n",
      "ActionParsnip    1182\n",
      "Name: to, dtype: int64\n",
      "\n",
      "Top 5 Users with answers:\n",
      "bazhang          5278\n",
      "ActionParsnip    5010\n",
      "jrib             4586\n",
      "Pici             4297\n",
      "ikonia           4069\n",
      "Name: from, dtype: int64\n",
      "\n",
      "Top 5 User with Questions:\n",
      "ActionParsnip    49727\n",
      "ikonia           39167\n",
      "jrib             33973\n",
      "Dr_Willis        22160\n",
      "bazhang          21404\n",
      "Name: to, dtype: int64\n",
      "\n",
      "Top 5 Users with answers:\n",
      "ActionParsnip    97134\n",
      "ikonia           77058\n",
      "jrib             51863\n",
      "Dr_Willis        44352\n",
      "bazhang          40881\n",
      "Name: from, dtype: int64\n",
      "\n",
      "Top 5 User with Questions:\n",
      "ActionParsnip    90543\n",
      "ikonia           71422\n",
      "jrib             63193\n",
      "Dr_Willis        41614\n",
      "bazhang          39137\n",
      "Name: to, dtype: int64\n",
      "\n",
      "Top 5 Users with answers:\n",
      "ActionParsnip    174906\n",
      "ikonia           140263\n",
      "jrib              96249\n",
      "Dr_Willis         81889\n",
      "bazhang           74845\n",
      "Name: from, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Finding the top 5 most common user IDs in each set\n",
    "top_5_from1 = ubun1['to'].value_counts().head(5)\n",
    "top_5_to1 = ubun1['from'].value_counts().head(5)\n",
    "\n",
    "print(\"Top 5 User with Questions:\")\n",
    "print(top_5_from1)\n",
    "print(\"\\nTop 5 Users with answers:\")\n",
    "print(top_5_to1)\n",
    "\n",
    "top_5_from2 = ubun2['to'].value_counts().head(5)\n",
    "top_5_to2 = ubun2['from'].value_counts().head(5)\n",
    "\n",
    "print(\"\\nTop 5 User with Questions:\")\n",
    "print(top_5_from2)\n",
    "print(\"\\nTop 5 Users with answers:\")\n",
    "print(top_5_to2)\n",
    "\n",
    "top_5_from3 = ubun3['to'].value_counts().head(5)\n",
    "top_5_to3 = ubun3['from'].value_counts().head(5)\n",
    "\n",
    "print(\"\\nTop 5 User with Questions:\")\n",
    "print(top_5_from3)\n",
    "print(\"\\nTop 5 Users with answers:\")\n",
    "print(top_5_to3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfef1ac",
   "metadata": {},
   "source": [
    "So looks like 'ActionParsnip' is very popular, with both questions and answers. 'Ubuntu' itself is not as prevalent as we would think, but taking into account the replies from above users will definetly help us gain an understanding of communication by each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1767511a",
   "metadata": {},
   "source": [
    "Now lets look at the most common words in each of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1f5d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abbaspardawalla/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import all the libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a245e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "folder             0\n",
       "dialogueID         0\n",
       "date               0\n",
       "from              13\n",
       "to            472204\n",
       "text               0\n",
       "Tokens             0\n",
       "LongWords          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From earlier we knew, we had few null values in our text column. Lets drop those to tokenise our values easier\n",
    "ubun1 = ubun1.dropna(subset=['text'])\n",
    "ubun1.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97f92941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ubuntu', 78633), ('install', 59607), ('anyone', 41321), ('Ubuntu', 29528), ('thanks', 25051), ('installed', 24598), ('windows', 21989), ('should', 21866), ('command', 19373), ('problem', 19168)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "ubun1['Tokens'] = ubun1['text'].apply(word_tokenize)\n",
    "\n",
    "# Filter for words longer than 5 letters\n",
    "ubun1['LongWords'] = ubun1['Tokens'].apply(lambda tokens: [word for word in tokens if len(word) > 5])\n",
    "\n",
    "# Flatten the list of long words\n",
    "all_long_words1 = [word for words in ubun1['LongWords'] for word in words]\n",
    "\n",
    "# Calculate word frequencies for long words\n",
    "long_word_freq = FreqDist(all_long_words)\n",
    "\n",
    "# Display the most common long words\n",
    "print(long_word_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df13b41a",
   "metadata": {},
   "source": [
    "Now lets do the painstaking work in finding the most common words in the other two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a9ba4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "folder              0\n",
       "dialogueID          0\n",
       "date                0\n",
       "from              202\n",
       "to            3294506\n",
       "text                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ubun2 = ubun2.dropna(subset=['text'])\n",
    "ubun2.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "ubun2['Tokens'] = ubun2['text'].apply(word_tokenize)\n",
    "\n",
    "# Filter for words longer than 5 letters\n",
    "ubun2['LongWords'] = ubun2['Tokens'].apply(lambda tokens: [word for word in tokens if len(word) > 5])\n",
    "\n",
    "# Flatten the list of long words\n",
    "all_long_words2 = [word for words in ubun2['LongWords'] for word in words]\n",
    "\n",
    "# Calculate word frequencies for long words\n",
    "long_word_freq2 = FreqDist(all_long_words2)\n",
    "\n",
    "# Display the most common long words\n",
    "print(long_word_freq2.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd34d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubun3 = ubun3.dropna(subset=['text'])\n",
    "ubun3.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "ubun3['Tokens'] = ubun3['text'].apply(word_tokenize)\n",
    "\n",
    "# Filter for words longer than 5 letters\n",
    "ubun3['LongWords'] = ubun3['Tokens'].apply(lambda tokens: [word for word in tokens if len(word) > 5])\n",
    "\n",
    "# Flatten the list of long words\n",
    "all_long_words3 = [word for words in ubun3['LongWords'] for word in words]\n",
    "\n",
    "# Calculate word frequencies for long words\n",
    "long_word_freq3 = FreqDist(all_long_words3)\n",
    "\n",
    "# Display the most common long words\n",
    "print(long_word_freq3.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ddc464",
   "metadata": {},
   "source": [
    "Cautionary Tale - This EDA is an ongoing process. While the model is in play , we will have to keep on referring to this notebook, as well as adding and needing additional info which hadnt been thought of previously. New commitments to the dataset will always be ongoing , along with new features that will be added to the preliminary task. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
